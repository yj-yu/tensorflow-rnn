{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN HANGUL-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from TextLoader import *\n",
    "from Hangulpy import *\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET WITH TEXTLOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"invisible_dragon\" # \"nine_dreams\"\n",
    "\n",
    "data_dir    = \"data/\" + corpus_name\n",
    "batch_size  = 10\n",
    "seq_length  = 100\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOCAB AND CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of 'data_loader.vocab' is <type 'dict'>, length is 84\n",
      "type of 'data_loader.chars' is <type 'tuple'>, length is 84\n"
     ]
    }
   ],
   "source": [
    "vocab_size = data_loader.vocab_size\n",
    "vocab = data_loader.vocab\n",
    "chars = data_loader.chars\n",
    "print ( \"type of 'data_loader.vocab' is %s, length is %d\" \n",
    "       % (type(data_loader.vocab), len(data_loader.vocab)) )\n",
    "print ( \"type of 'data_loader.chars' is %s, length is %d\" \n",
    "       % (type(data_loader.chars), len(data_loader.chars)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOCAB: DICTIONARY (CHAR->INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_': 81, u'6': 63, u'|': 83, u'\\n': 6, u'\\r': 7, u',': 74, u'x': 75, u';': 73, u'[': 71, u'\\u3144': 60, u'!': 28, u' ': 2, u'#': 68, u'\"': 34, u'\\u1d25': 0, u\"'\": 65, u')': 50, u'(': 51, u'+': 76, u'*': 82, u']': 72, u'\\u3133': 58, u'/': 45, u'.': 24, u'\\u3131': 5, u'0': 27, u'3': 54, u'2': 36, u'5': 61, u'\\u3134': 4, u'\\u3137': 11, u'\\u3136': 49, u'\\u3139': 8, u'\\u3138': 31, u'\\u3156': 52, u':': 40, u'\\u313c': 67, u'?': 44, u'4': 59, u'\\u3141': 14, u'\\u3140': 77, u'\\u3143': 57, u'\\u3142': 21, u'\\u3145': 15, u'7': 47, u'\\u3147': 1, u'\\u3146': 22, u'\\u3149': 38, u'\\u3148': 16, u'\\u314b': 26, u'\\u314a': 30, u'\\u314d': 33, u'\\u314c': 25, u'\\u314f': 3, u'\\u314e': 19, u'\\u3151': 32, u'\\u3150': 18, u'\\u3153': 13, u'\\u3152': 69, u'\\u3155': 20, u'\\u3154': 23, u'\\u3157': 12, u'8': 46, u'\\u3159': 62, u'\\u3158': 41, u'\\u315b': 29, u'\\u315a': 53, u'\\u315d': 48, u'\\u315c': 17, u'\\u315f': 39, u'^': 64, u'\\u3161': 10, u'\\u3160': 55, u'\\u3163': 9, u'\\u3162': 43, u'k': 78, u'9': 35, u'\\u313a': 70, u'1': 56, u'\\u3132': 42, u'%': 66, u'}': 80, u'<': 79, u'~': 37}\n"
     ]
    }
   ],
   "source": [
    "print (data_loader.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARS: LIST (INDEX->CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\u1d25', u'\\u3147', u' ', u'\\u314f', u'\\u3134', u'\\u3131', u'\\n', u'\\r', u'\\u3139', u'\\u3163', u'\\u3161', u'\\u3137', u'\\u3157', u'\\u3153', u'\\u3141', u'\\u3145', u'\\u3148', u'\\u315c', u'\\u3150', u'\\u314e', u'\\u3155', u'\\u3142', u'\\u3146', u'\\u3154', u'.', u'\\u314c', u'\\u314b', u'0', u'!', u'\\u315b', u'\\u314a', u'\\u3138', u'\\u3151', u'\\u314d', u'\"', u'9', u'2', u'~', u'\\u3149', u'\\u315f', u':', u'\\u3158', u'\\u3132', u'\\u3162', u'?', u'/', u'8', u'7', u'\\u315d', u'\\u3136', u')', u'(', u'\\u3156', u'\\u315a', u'3', u'\\u3160', u'1', u'\\u3143', u'\\u3133', u'4', u'\\u3144', u'5', u'\\u3159', u'6', u'^', u\"'\", u'%', u'\\u313c', u'#', u'\\u3152', u'\\u313a', u'[', u']', u';', u',', u'x', u'+', u'\\u3140', u'k', u'<', u'}', u'_', u'*', u'|')\n",
      "á´¥\n"
     ]
    }
   ],
   "source": [
    "print (data_loader.chars)\n",
    "# USAGE\n",
    "print (data_loader.chars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING BATCH (IMPORTANT!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'x' is <type 'numpy.ndarray'>. Shape is (10, 100)\n",
      "x looks like \n",
      "[[ 7  6  1 20  0  8 13  0 21 10  4  0  2 16 18  0  5  3  0  2 11 10  0 11\n",
      "   9  0  1 13  0  2  5 10  8  0  1 10  8  0  2 22 10 14  0  4  9  0 11  3\n",
      "   0 37  7  6 14  3  4  0  1  9  0  2 21 41  0 16 17  0 15 18  0  1 29  0\n",
      "  37  7  6  4 18  0  1 29  1  0  1 10  4  0  2 30 53  0  5  3  1  0  1 43\n",
      "   0  2 11 10]\n",
      " [23  0 34  7  6  7  6  2  7  6  7  6  5 10  0  8 13  0 16  3  0  2 16 13\n",
      "   0  5 10  8  0  8  9  1  0  5 41  0 19  9  0 11 10  0  8  3  0 11 10  8\n",
      "   0  1 10  4  0  2 11 17  0  8 20  0  1 17 14  0  1 23  0 14 12 14  0  1\n",
      "  10  8  0  2 31 13  8  0  1 13 22  0 11  3  0  7  6  5 10  4  0 11 23  0\n",
      "   1  3  4  0]\n",
      " [ 1 13 21  0 15 13 22  0 11  3  0  7  6 25 17  0 14 20  1  0 19 18  0 15\n",
      "  13  0  1  3  4  0 21 12  0  1 20  0 15 13  0  2  5 10  0  8 18  0 15 13\n",
      "   0  2 11 13  0 14 17  0 15 13  0  1 48  0 15 13  0 16 12  8  0  8  3  0\n",
      "   2 14 12 14  0  1 10  8  0  2 14  3  5  0  2 14 17  0 15 13  0  1 48  0\n",
      "  15 13  0  7]\n",
      " [ 5 18  0 21  3  8  0 33  9  8  0 15  3  8  0  5  9  0  1  9  4  0  2 16\n",
      "   3  0 33 12  5  0 33  9  0 19  3  0  5  9  0  1 20 15  0 11  3  0  2 19\n",
      "   3  0 16  9  0 14  3  4  0  1  9  0  5 13  4  0 11  3  4  0 16 13 14  0\n",
      "   1  9  0  2  1  9 22  0  1 13 15  0 11  3  0  7  6  5 10  0  5 18  0 14\n",
      "  48  0  4 32]\n",
      " [ 0  2  5 13 15  0  1  9  0  1 13 15  0 11  3  0  7  6 19  3  0 16  9  0\n",
      "  14  3  4  0  2 25 17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0  1  9\n",
      "   0  2  5 10  0 31 18  0 11 39  0 26 10  0  8 10  8  0 21 12  0  5 12  0\n",
      "   2 15  3  8  0  8 20  0 11 17  8  0  8  9  0  5  3  0  2  1 13 21  0  1\n",
      "  13 15  0 11]\n",
      " [ 0  1  3  8  0  1 10  8  0  2 42 18  0  5 12  0  4  3  0  1 41 15  0  4\n",
      "  10  4  0 11 23  0  2  1 13 14  0 14  3  0 19  3  0  5 12  0  1  3  0 57\n",
      "   3  0  5  3  0  1 13 21  0 15 13 15  0 11  3  0  7  6  5 10  0  8 18  0\n",
      "  15 13  0 25 17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0  1 10  4  0\n",
      "   2 15 10  8]\n",
      " [55  5  0 21 18  5  0 14 20  1  0  1 10  4  0 15 17  4  0 15  9  5  0  5\n",
      "   3  4  0  1 23  0  2 25 17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0\n",
      "   1 43  0  2 16 17  0  1 39  0  8 10  8  0  7  6 11 17  8  0  8 13  0 22\n",
      "   3 22  0  5 12  0  2 25 17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0\n",
      "   1 10  4  0]\n",
      " [ 5 12  0  2  1  9  0  2  1  3  4  0 21 12  0  1  9  0  4 10  4  0  2 25\n",
      "  17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0 11 12  0  2  5 10  0  8\n",
      "  13 19  0  5 12  0  7  6 11  3  0  2 21 12  5  0 15 17  0 19  3  0  8 13\n",
      "   0  2 31 13  0  4  3  4  0 11  3  0  5 12  0 19  3  0  5 12  0 24 24 24\n",
      "   7  6  7  6]\n",
      " [ 7  6  7  6  5 10  0 31 18  0  2 25 17  0 14 20  1  0 11 10  0  8 18  0\n",
      "   5 12  4  0  1 10  4  0  2 16  9  4  0 38  3  0  2 16 12  8  0  8  3  0\n",
      "   2 38  3  1  0  2 14 13 15  0  1  9 22  0  5 23  0  2 19  3  4  0 14  3\n",
      "   0 11  9  0  8 10  8  0  2  4 18  0 21 18 25  0  1 13 22  0 11  3  0 28\n",
      "   7  6  7  6]\n",
      " [17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0 11 12  0  2  1  9  0 16\n",
      "  23  0  2 16 17  5  0  1 13  0  5  3  0  5 12  0  2  1  9 22  0  1 13 22\n",
      "   0 11  3  0 24 24 24 24 24 24 24 24  7  6  1 13 21  0  5 10  0  8 23  0\n",
      "   1  9  0 15 20  4  0  2 16  3  0 33 12  5  0 33  9  0 19  3  0  5  9  0\n",
      "   8 12  0  2]]\n",
      "\n",
      "Type of 'y' is <type 'numpy.ndarray'>. Shape is (10, 100)\n",
      "y looks like \n",
      "[[ 6  1 20  0  8 13  0 21 10  4  0  2 16 18  0  5  3  0  2 11 10  0 11  9\n",
      "   0  1 13  0  2  5 10  8  0  1 10  8  0  2 22 10 14  0  4  9  0 11  3  0\n",
      "  37  7  6 14  3  4  0  1  9  0  2 21 41  0 16 17  0 15 18  0  1 29  0 37\n",
      "   7  6  4 18  0  1 29  1  0  1 10  4  0  2 30 53  0  5  3  1  0  1 43  0\n",
      "   2 11 10  0]\n",
      " [ 0 34  7  6  7  6  2  7  6  7  6  5 10  0  8 13  0 16  3  0  2 16 13  0\n",
      "   5 10  8  0  8  9  1  0  5 41  0 19  9  0 11 10  0  8  3  0 11 10  8  0\n",
      "   1 10  4  0  2 11 17  0  8 20  0  1 17 14  0  1 23  0 14 12 14  0  1 10\n",
      "   8  0  2 31 13  8  0  1 13 22  0 11  3  0  7  6  5 10  4  0 11 23  0  1\n",
      "   3  4  0 21]\n",
      " [13 21  0 15 13 22  0 11  3  0  7  6 25 17  0 14 20  1  0 19 18  0 15 13\n",
      "   0  1  3  4  0 21 12  0  1 20  0 15 13  0  2  5 10  0  8 18  0 15 13  0\n",
      "   2 11 13  0 14 17  0 15 13  0  1 48  0 15 13  0 16 12  8  0  8  3  0  2\n",
      "  14 12 14  0  1 10  8  0  2 14  3  5  0  2 14 17  0 15 13  0  1 48  0 15\n",
      "  13  0  7  6]\n",
      " [18  0 21  3  8  0 33  9  8  0 15  3  8  0  5  9  0  1  9  4  0  2 16  3\n",
      "   0 33 12  5  0 33  9  0 19  3  0  5  9  0  1 20 15  0 11  3  0  2 19  3\n",
      "   0 16  9  0 14  3  4  0  1  9  0  5 13  4  0 11  3  4  0 16 13 14  0  1\n",
      "   9  0  2  1  9 22  0  1 13 15  0 11  3  0  7  6  5 10  0  5 18  0 14 48\n",
      "   0  4 32  0]\n",
      " [ 2  5 13 15  0  1  9  0  1 13 15  0 11  3  0  7  6 19  3  0 16  9  0 14\n",
      "   3  4  0  2 25 17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0  1  9  0\n",
      "   2  5 10  0 31 18  0 11 39  0 26 10  0  8 10  8  0 21 12  0  5 12  0  2\n",
      "  15  3  8  0  8 20  0 11 17  8  0  8  9  0  5  3  0  2  1 13 21  0  1 13\n",
      "  15  0 11  3]\n",
      " [ 1  3  8  0  1 10  8  0  2 42 18  0  5 12  0  4  3  0  1 41 15  0  4 10\n",
      "   4  0 11 23  0  2  1 13 14  0 14  3  0 19  3  0  5 12  0  1  3  0 57  3\n",
      "   0  5  3  0  1 13 21  0 15 13 15  0 11  3  0  7  6  5 10  0  8 18  0 15\n",
      "  13  0 25 17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0  1 10  4  0  2\n",
      "  15 10  8  0]\n",
      " [ 5  0 21 18  5  0 14 20  1  0  1 10  4  0 15 17  4  0 15  9  5  0  5  3\n",
      "   4  0  1 23  0  2 25 17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0  1\n",
      "  43  0  2 16 17  0  1 39  0  8 10  8  0  7  6 11 17  8  0  8 13  0 22  3\n",
      "  22  0  5 12  0  2 25 17  0 14 20  1  0 11 10  0  8 18  0  5 12  4  0  1\n",
      "  10  4  0  2]\n",
      " [12  0  2  1  9  0  2  1  3  4  0 21 12  0  1  9  0  4 10  4  0  2 25 17\n",
      "   0 14 20  1  0 11 10  0  8 18  0  5 12  4  0 11 12  0  2  5 10  0  8 13\n",
      "  19  0  5 12  0  7  6 11  3  0  2 21 12  5  0 15 17  0 19  3  0  8 13  0\n",
      "   2 31 13  0  4  3  4  0 11  3  0  5 12  0 19  3  0  5 12  0 24 24 24  7\n",
      "   6  7  6  2]\n",
      " [ 6  7  6  5 10  0 31 18  0  2 25 17  0 14 20  1  0 11 10  0  8 18  0  5\n",
      "  12  4  0  1 10  4  0  2 16  9  4  0 38  3  0  2 16 12  8  0  8  3  0  2\n",
      "  38  3  1  0  2 14 13 15  0  1  9 22  0  5 23  0  2 19  3  4  0 14  3  0\n",
      "  11  9  0  8 10  8  0  2  4 18  0 21 18 25  0  1 13 22  0 11  3  0 28  7\n",
      "   6  7  6  2]\n",
      " [ 0 14 20  1  0 11 10  0  8 18  0  5 12  4  0 11 12  0  2  1  9  0 16 23\n",
      "   0  2 16 17  5  0  1 13  0  5  3  0  5 12  0  2  1  9 22  0  1 13 22  0\n",
      "  11  3  0 24 24 24 24 24 24 24 24  7  6  1 13 21  0  5 10  0  8 23  0  1\n",
      "   9  0 15 20  4  0  2 16  3  0 33 12  5  0 33  9  0 19  3  0  5  9  0  8\n",
      "  12  0  2 19]]\n"
     ]
    }
   ],
   "source": [
    "x, y = data_loader.next_batch()\n",
    "print (\"Type of 'x' is %s. Shape is %s\" % (type(x), x.shape,))\n",
    "print (\"x looks like \\n%s\" % (x))\n",
    "print\n",
    "print (\"Type of 'y' is %s. Shape is %s\" % (type(y), y.shape,))\n",
    "print (\"y looks like \\n%s\" % (y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE A MULTILAYER LSTM NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network ready\n"
     ]
    }
   ],
   "source": [
    "rnn_size   = 128\n",
    "num_layers = 2\n",
    "grad_clip  = 5. # <= GRADIENT CLIPPING (PRACTICALLY IMPORTANT)\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "# SELECT RNN CELL (MULTI LAYER LSTM)\n",
    "def unit_cell():\n",
    "    return tf.contrib.rnn.BasicLSTMCell(rnn_size,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([unit_cell() for _ in range(num_layers)])\n",
    "\n",
    "# Set paths to the graph\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets    = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# Set Network\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "        inputs = tf.split(tf.nn.embedding_lookup(embedding, input_data), seq_length, 1)\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "print (\"Network ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONS READY\n"
     ]
    }
   ],
   "source": [
    "# Output of RNN\n",
    "outputs, last_state = tf.contrib.rnn.static_rnn(cell,inputs, initial_state, \n",
    "                                                scope='rnnlm')\n",
    "\n",
    "output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "\n",
    "# Next word probability\n",
    "probs = tf.nn.softmax(logits)\n",
    "print (\"FUNCTIONS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE LOSS FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS FUNCTION\n"
     ]
    }
   ],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], # Input\n",
    "    [tf.reshape(targets, [-1])], # Target\n",
    "    [tf.ones([batch_size * seq_length])], # Weight\n",
    "    vocab_size)\n",
    "print (\"LOSS FUNCTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE COST FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK READY\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "\n",
    "# GRADIENT CLIPPING ! \n",
    "lr = tf.Variable(0.0, trainable=False) # <= LEARNING RATE \n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "_optm = tf.train.AdamOptimizer(lr)\n",
    "optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "final_state = last_state\n",
    "print (\"NETWORK READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMIZE NETWORK WITH LR SCHEDULING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yj/.virtualenvs/lecture/local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_epochs    = 5000\n",
    "save_every    = 2000\n",
    "learning_rate = 0.001\n",
    "decay_rate    = 0.999\n",
    "\n",
    "save_dir = 'data/' + corpus_name\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-36cc09519909>:4: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "0/325000 (epoch: 0), loss: 4.437, time/batch: 0.968\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "65/325000 (epoch: 1), loss: 3.019, time/batch: 0.074\n",
      "130/325000 (epoch: 2), loss: 2.194, time/batch: 0.073\n",
      "195/325000 (epoch: 3), loss: 1.977, time/batch: 0.072\n",
      "260/325000 (epoch: 4), loss: 1.880, time/batch: 0.087\n",
      "325/325000 (epoch: 5), loss: 1.791, time/batch: 0.073\n",
      "390/325000 (epoch: 6), loss: 1.725, time/batch: 0.071\n",
      "455/325000 (epoch: 7), loss: 1.669, time/batch: 0.071\n",
      "520/325000 (epoch: 8), loss: 1.621, time/batch: 0.072\n",
      "585/325000 (epoch: 9), loss: 1.576, time/batch: 0.093\n",
      "650/325000 (epoch: 10), loss: 1.538, time/batch: 0.085\n",
      "715/325000 (epoch: 11), loss: 1.502, time/batch: 0.081\n",
      "780/325000 (epoch: 12), loss: 1.469, time/batch: 0.071\n",
      "845/325000 (epoch: 13), loss: 1.443, time/batch: 0.084\n",
      "910/325000 (epoch: 14), loss: 1.420, time/batch: 0.072\n",
      "975/325000 (epoch: 15), loss: 1.398, time/batch: 0.091\n",
      "1040/325000 (epoch: 16), loss: 1.379, time/batch: 0.073\n",
      "1105/325000 (epoch: 17), loss: 1.362, time/batch: 0.079\n",
      "1170/325000 (epoch: 18), loss: 1.346, time/batch: 0.079\n",
      "1235/325000 (epoch: 19), loss: 1.330, time/batch: 0.087\n",
      "1300/325000 (epoch: 20), loss: 1.315, time/batch: 0.161\n",
      "1365/325000 (epoch: 21), loss: 1.298, time/batch: 0.081\n",
      "1430/325000 (epoch: 22), loss: 1.282, time/batch: 0.084\n",
      "1495/325000 (epoch: 23), loss: 1.267, time/batch: 0.077\n",
      "1560/325000 (epoch: 24), loss: 1.253, time/batch: 0.085\n",
      "1625/325000 (epoch: 25), loss: 1.239, time/batch: 0.071\n",
      "1690/325000 (epoch: 26), loss: 1.225, time/batch: 0.077\n",
      "1755/325000 (epoch: 27), loss: 1.211, time/batch: 0.075\n",
      "1820/325000 (epoch: 28), loss: 1.195, time/batch: 0.073\n",
      "1885/325000 (epoch: 29), loss: 1.181, time/batch: 0.081\n",
      "1950/325000 (epoch: 30), loss: 1.166, time/batch: 0.072\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "2015/325000 (epoch: 31), loss: 1.151, time/batch: 0.071\n",
      "2080/325000 (epoch: 32), loss: 1.136, time/batch: 0.076\n",
      "2145/325000 (epoch: 33), loss: 1.123, time/batch: 0.101\n",
      "2210/325000 (epoch: 34), loss: 1.110, time/batch: 0.072\n",
      "2275/325000 (epoch: 35), loss: 1.096, time/batch: 0.075\n",
      "2340/325000 (epoch: 36), loss: 1.082, time/batch: 0.073\n",
      "2405/325000 (epoch: 37), loss: 1.070, time/batch: 0.072\n",
      "2470/325000 (epoch: 38), loss: 1.057, time/batch: 0.078\n",
      "2535/325000 (epoch: 39), loss: 1.048, time/batch: 0.106\n",
      "2600/325000 (epoch: 40), loss: 1.032, time/batch: 0.075\n",
      "2665/325000 (epoch: 41), loss: 1.021, time/batch: 0.076\n",
      "2730/325000 (epoch: 42), loss: 1.012, time/batch: 0.076\n",
      "2795/325000 (epoch: 43), loss: 0.999, time/batch: 0.075\n",
      "2860/325000 (epoch: 44), loss: 0.985, time/batch: 0.081\n",
      "2925/325000 (epoch: 45), loss: 0.975, time/batch: 0.071\n",
      "2990/325000 (epoch: 46), loss: 0.964, time/batch: 0.080\n",
      "3055/325000 (epoch: 47), loss: 0.958, time/batch: 0.079\n",
      "3120/325000 (epoch: 48), loss: 0.947, time/batch: 0.071\n",
      "3185/325000 (epoch: 49), loss: 0.936, time/batch: 0.078\n",
      "3250/325000 (epoch: 50), loss: 0.924, time/batch: 0.073\n",
      "3315/325000 (epoch: 51), loss: 0.909, time/batch: 0.070\n",
      "3380/325000 (epoch: 52), loss: 0.897, time/batch: 0.075\n",
      "3445/325000 (epoch: 53), loss: 0.882, time/batch: 0.077\n",
      "3510/325000 (epoch: 54), loss: 0.875, time/batch: 0.076\n",
      "3575/325000 (epoch: 55), loss: 0.865, time/batch: 0.076\n",
      "3640/325000 (epoch: 56), loss: 0.857, time/batch: 0.106\n",
      "3705/325000 (epoch: 57), loss: 0.844, time/batch: 0.079\n",
      "3770/325000 (epoch: 58), loss: 0.835, time/batch: 0.073\n",
      "3835/325000 (epoch: 59), loss: 0.823, time/batch: 0.076\n",
      "3900/325000 (epoch: 60), loss: 0.813, time/batch: 0.071\n",
      "3965/325000 (epoch: 61), loss: 0.803, time/batch: 0.071\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "4030/325000 (epoch: 62), loss: 0.792, time/batch: 0.073\n",
      "4095/325000 (epoch: 63), loss: 0.783, time/batch: 0.073\n",
      "4160/325000 (epoch: 64), loss: 0.775, time/batch: 0.102\n",
      "4225/325000 (epoch: 65), loss: 0.775, time/batch: 0.088\n",
      "4290/325000 (epoch: 66), loss: 0.763, time/batch: 0.070\n",
      "4355/325000 (epoch: 67), loss: 0.755, time/batch: 0.075\n",
      "4420/325000 (epoch: 68), loss: 0.745, time/batch: 0.071\n",
      "4485/325000 (epoch: 69), loss: 0.743, time/batch: 0.103\n",
      "4550/325000 (epoch: 70), loss: 0.737, time/batch: 0.086\n",
      "4615/325000 (epoch: 71), loss: 0.731, time/batch: 0.072\n",
      "4680/325000 (epoch: 72), loss: 0.722, time/batch: 0.075\n",
      "4745/325000 (epoch: 73), loss: 0.709, time/batch: 0.075\n",
      "4810/325000 (epoch: 74), loss: 0.704, time/batch: 0.078\n",
      "4875/325000 (epoch: 75), loss: 0.706, time/batch: 0.076\n",
      "4940/325000 (epoch: 76), loss: 0.717, time/batch: 0.074\n",
      "5005/325000 (epoch: 77), loss: 0.711, time/batch: 0.075\n",
      "5070/325000 (epoch: 78), loss: 0.697, time/batch: 0.070\n",
      "5135/325000 (epoch: 79), loss: 0.686, time/batch: 0.104\n",
      "5200/325000 (epoch: 80), loss: 0.685, time/batch: 0.086\n",
      "5265/325000 (epoch: 81), loss: 0.681, time/batch: 0.070\n",
      "5330/325000 (epoch: 82), loss: 0.673, time/batch: 0.076\n",
      "5395/325000 (epoch: 83), loss: 0.663, time/batch: 0.073\n",
      "5460/325000 (epoch: 84), loss: 0.661, time/batch: 0.072\n",
      "5525/325000 (epoch: 85), loss: 0.662, time/batch: 0.074\n",
      "5590/325000 (epoch: 86), loss: 0.641, time/batch: 0.072\n",
      "5655/325000 (epoch: 87), loss: 0.630, time/batch: 0.077\n",
      "5720/325000 (epoch: 88), loss: 0.618, time/batch: 0.077\n",
      "5785/325000 (epoch: 89), loss: 0.606, time/batch: 0.071\n",
      "5850/325000 (epoch: 90), loss: 0.599, time/batch: 0.085\n",
      "5915/325000 (epoch: 91), loss: 0.595, time/batch: 0.071\n",
      "5980/325000 (epoch: 92), loss: 0.586, time/batch: 0.072\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "6045/325000 (epoch: 93), loss: 0.581, time/batch: 0.074\n",
      "6110/325000 (epoch: 94), loss: 0.573, time/batch: 0.083\n",
      "6175/325000 (epoch: 95), loss: 0.566, time/batch: 0.081\n",
      "6240/325000 (epoch: 96), loss: 0.560, time/batch: 0.073\n",
      "6305/325000 (epoch: 97), loss: 0.551, time/batch: 0.072\n",
      "6370/325000 (epoch: 98), loss: 0.541, time/batch: 0.076\n",
      "6435/325000 (epoch: 99), loss: 0.536, time/batch: 0.073\n",
      "6500/325000 (epoch: 100), loss: 0.529, time/batch: 0.072\n",
      "6565/325000 (epoch: 101), loss: 0.517, time/batch: 0.079\n",
      "6630/325000 (epoch: 102), loss: 0.510, time/batch: 0.073\n",
      "6695/325000 (epoch: 103), loss: 0.505, time/batch: 0.081\n",
      "6760/325000 (epoch: 104), loss: 0.502, time/batch: 0.076\n",
      "6825/325000 (epoch: 105), loss: 0.492, time/batch: 0.102\n",
      "6890/325000 (epoch: 106), loss: 0.493, time/batch: 0.081\n",
      "6955/325000 (epoch: 107), loss: 0.488, time/batch: 0.072\n",
      "7020/325000 (epoch: 108), loss: 0.488, time/batch: 0.077\n",
      "7085/325000 (epoch: 109), loss: 0.484, time/batch: 0.071\n",
      "7150/325000 (epoch: 110), loss: 0.488, time/batch: 0.072\n",
      "7215/325000 (epoch: 111), loss: 0.477, time/batch: 0.077\n",
      "7280/325000 (epoch: 112), loss: 0.469, time/batch: 0.075\n",
      "7345/325000 (epoch: 113), loss: 0.470, time/batch: 0.074\n",
      "7410/325000 (epoch: 114), loss: 0.476, time/batch: 0.077\n",
      "7475/325000 (epoch: 115), loss: 0.482, time/batch: 0.088\n",
      "7540/325000 (epoch: 116), loss: 0.486, time/batch: 0.086\n",
      "7605/325000 (epoch: 117), loss: 0.480, time/batch: 0.071\n",
      "7670/325000 (epoch: 118), loss: 0.463, time/batch: 0.072\n",
      "7735/325000 (epoch: 119), loss: 0.446, time/batch: 0.080\n",
      "7800/325000 (epoch: 120), loss: 0.449, time/batch: 0.074\n",
      "7865/325000 (epoch: 121), loss: 0.441, time/batch: 0.075\n",
      "7930/325000 (epoch: 122), loss: 0.433, time/batch: 0.079\n",
      "7995/325000 (epoch: 123), loss: 0.415, time/batch: 0.074\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "8060/325000 (epoch: 124), loss: 0.405, time/batch: 0.076\n",
      "8125/325000 (epoch: 125), loss: 0.397, time/batch: 0.077\n",
      "8190/325000 (epoch: 126), loss: 0.395, time/batch: 0.102\n",
      "8255/325000 (epoch: 127), loss: 0.386, time/batch: 0.079\n",
      "8320/325000 (epoch: 128), loss: 0.391, time/batch: 0.074\n",
      "8385/325000 (epoch: 129), loss: 0.386, time/batch: 0.077\n",
      "8450/325000 (epoch: 130), loss: 0.388, time/batch: 0.076\n",
      "8515/325000 (epoch: 131), loss: 0.381, time/batch: 0.103\n",
      "8580/325000 (epoch: 132), loss: 0.379, time/batch: 0.075\n",
      "8645/325000 (epoch: 133), loss: 0.381, time/batch: 0.072\n",
      "8710/325000 (epoch: 134), loss: 0.389, time/batch: 0.078\n",
      "8775/325000 (epoch: 135), loss: 0.384, time/batch: 0.073\n",
      "8840/325000 (epoch: 136), loss: 0.382, time/batch: 0.073\n",
      "8905/325000 (epoch: 137), loss: 0.378, time/batch: 0.089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8970/325000 (epoch: 138), loss: 0.369, time/batch: 0.073\n",
      "9035/325000 (epoch: 139), loss: 0.364, time/batch: 0.075\n",
      "9100/325000 (epoch: 140), loss: 0.371, time/batch: 0.077\n",
      "9165/325000 (epoch: 141), loss: 0.382, time/batch: 0.074\n",
      "9230/325000 (epoch: 142), loss: 0.369, time/batch: 0.073\n",
      "9295/325000 (epoch: 143), loss: 0.367, time/batch: 0.076\n",
      "9360/325000 (epoch: 144), loss: 0.364, time/batch: 0.072\n",
      "9425/325000 (epoch: 145), loss: 0.350, time/batch: 0.075\n",
      "9490/325000 (epoch: 146), loss: 0.346, time/batch: 0.079\n",
      "9555/325000 (epoch: 147), loss: 0.331, time/batch: 0.073\n",
      "9620/325000 (epoch: 148), loss: 0.325, time/batch: 0.077\n",
      "9685/325000 (epoch: 149), loss: 0.317, time/batch: 0.073\n",
      "9750/325000 (epoch: 150), loss: 0.319, time/batch: 0.074\n",
      "9815/325000 (epoch: 151), loss: 0.321, time/batch: 0.082\n",
      "9880/325000 (epoch: 152), loss: 0.312, time/batch: 0.074\n",
      "9945/325000 (epoch: 153), loss: 0.323, time/batch: 0.100\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "10010/325000 (epoch: 154), loss: 0.323, time/batch: 0.078\n",
      "10075/325000 (epoch: 155), loss: 0.324, time/batch: 0.075\n",
      "10140/325000 (epoch: 156), loss: 0.321, time/batch: 0.078\n",
      "10205/325000 (epoch: 157), loss: 0.314, time/batch: 0.071\n",
      "10270/325000 (epoch: 158), loss: 0.311, time/batch: 0.073\n",
      "10335/325000 (epoch: 159), loss: 0.320, time/batch: 0.077\n",
      "10400/325000 (epoch: 160), loss: 0.318, time/batch: 0.073\n",
      "10465/325000 (epoch: 161), loss: 0.313, time/batch: 0.072\n",
      "10530/325000 (epoch: 162), loss: 0.322, time/batch: 0.077\n",
      "10595/325000 (epoch: 163), loss: 0.311, time/batch: 0.075\n",
      "10660/325000 (epoch: 164), loss: 0.324, time/batch: 0.074\n",
      "10725/325000 (epoch: 165), loss: 0.304, time/batch: 0.080\n",
      "10790/325000 (epoch: 166), loss: 0.296, time/batch: 0.074\n",
      "10855/325000 (epoch: 167), loss: 0.294, time/batch: 0.078\n",
      "10920/325000 (epoch: 168), loss: 0.298, time/batch: 0.075\n",
      "10985/325000 (epoch: 169), loss: 0.280, time/batch: 0.074\n",
      "11050/325000 (epoch: 170), loss: 0.276, time/batch: 0.079\n",
      "11115/325000 (epoch: 171), loss: 0.271, time/batch: 0.079\n",
      "11180/325000 (epoch: 172), loss: 0.273, time/batch: 0.077\n",
      "11245/325000 (epoch: 173), loss: 0.272, time/batch: 0.078\n",
      "11310/325000 (epoch: 174), loss: 0.269, time/batch: 0.073\n",
      "11375/325000 (epoch: 175), loss: 0.262, time/batch: 0.073\n",
      "11440/325000 (epoch: 176), loss: 0.260, time/batch: 0.081\n",
      "11505/325000 (epoch: 177), loss: 0.260, time/batch: 0.074\n",
      "11570/325000 (epoch: 178), loss: 0.257, time/batch: 0.106\n",
      "11635/325000 (epoch: 179), loss: 0.257, time/batch: 0.087\n",
      "11700/325000 (epoch: 180), loss: 0.264, time/batch: 0.072\n",
      "11765/325000 (epoch: 181), loss: 0.281, time/batch: 0.077\n",
      "11830/325000 (epoch: 182), loss: 0.269, time/batch: 0.074\n",
      "11895/325000 (epoch: 183), loss: 0.277, time/batch: 0.072\n",
      "11960/325000 (epoch: 184), loss: 0.282, time/batch: 0.077\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "12025/325000 (epoch: 185), loss: 0.289, time/batch: 0.078\n",
      "12090/325000 (epoch: 186), loss: 0.272, time/batch: 0.073\n",
      "12155/325000 (epoch: 187), loss: 0.275, time/batch: 0.077\n",
      "12220/325000 (epoch: 188), loss: 0.262, time/batch: 0.082\n",
      "12285/325000 (epoch: 189), loss: 0.269, time/batch: 0.072\n",
      "12350/325000 (epoch: 190), loss: 0.276, time/batch: 0.077\n",
      "12415/325000 (epoch: 191), loss: 0.262, time/batch: 0.076\n",
      "12480/325000 (epoch: 192), loss: 0.246, time/batch: 0.072\n",
      "12545/325000 (epoch: 193), loss: 0.242, time/batch: 0.078\n",
      "12610/325000 (epoch: 194), loss: 0.239, time/batch: 0.072\n",
      "12675/325000 (epoch: 195), loss: 0.242, time/batch: 0.082\n",
      "12740/325000 (epoch: 196), loss: 0.228, time/batch: 0.078\n",
      "12805/325000 (epoch: 197), loss: 0.219, time/batch: 0.071\n",
      "12870/325000 (epoch: 198), loss: 0.215, time/batch: 0.075\n",
      "12935/325000 (epoch: 199), loss: 0.210, time/batch: 0.076\n",
      "13000/325000 (epoch: 200), loss: 0.202, time/batch: 0.103\n",
      "13065/325000 (epoch: 201), loss: 0.195, time/batch: 0.082\n",
      "13130/325000 (epoch: 202), loss: 0.199, time/batch: 0.075\n",
      "13195/325000 (epoch: 203), loss: 0.192, time/batch: 0.082\n",
      "13260/325000 (epoch: 204), loss: 0.185, time/batch: 0.085\n",
      "13325/325000 (epoch: 205), loss: 0.180, time/batch: 0.071\n",
      "13390/325000 (epoch: 206), loss: 0.180, time/batch: 0.081\n",
      "13455/325000 (epoch: 207), loss: 0.180, time/batch: 0.074\n",
      "13520/325000 (epoch: 208), loss: 0.177, time/batch: 0.072\n",
      "13585/325000 (epoch: 209), loss: 0.172, time/batch: 0.077\n",
      "13650/325000 (epoch: 210), loss: 0.185, time/batch: 0.078\n",
      "13715/325000 (epoch: 211), loss: 0.181, time/batch: 0.072\n",
      "13780/325000 (epoch: 212), loss: 0.175, time/batch: 0.078\n",
      "13845/325000 (epoch: 213), loss: 0.174, time/batch: 0.096\n",
      "13910/325000 (epoch: 214), loss: 0.179, time/batch: 0.080\n",
      "13975/325000 (epoch: 215), loss: 0.183, time/batch: 0.090\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "14040/325000 (epoch: 216), loss: 0.187, time/batch: 0.075\n",
      "14105/325000 (epoch: 217), loss: 0.188, time/batch: 0.076\n",
      "14170/325000 (epoch: 218), loss: 0.177, time/batch: 0.071\n",
      "14235/325000 (epoch: 219), loss: 0.173, time/batch: 0.072\n",
      "14300/325000 (epoch: 220), loss: 0.171, time/batch: 0.076\n",
      "14365/325000 (epoch: 221), loss: 0.163, time/batch: 0.071\n",
      "14430/325000 (epoch: 222), loss: 0.151, time/batch: 0.081\n",
      "14495/325000 (epoch: 223), loss: 0.151, time/batch: 0.080\n",
      "14560/325000 (epoch: 224), loss: 0.156, time/batch: 0.105\n",
      "14625/325000 (epoch: 225), loss: 0.158, time/batch: 0.082\n",
      "14690/325000 (epoch: 226), loss: 0.161, time/batch: 0.072\n",
      "14755/325000 (epoch: 227), loss: 0.152, time/batch: 0.077\n",
      "14820/325000 (epoch: 228), loss: 0.162, time/batch: 0.079\n",
      "14885/325000 (epoch: 229), loss: 0.152, time/batch: 0.073\n",
      "14950/325000 (epoch: 230), loss: 0.161, time/batch: 0.078\n",
      "15015/325000 (epoch: 231), loss: 0.156, time/batch: 0.084\n",
      "15080/325000 (epoch: 232), loss: 0.152, time/batch: 0.078\n",
      "15145/325000 (epoch: 233), loss: 0.164, time/batch: 0.098\n",
      "15210/325000 (epoch: 234), loss: 0.163, time/batch: 0.105\n",
      "15275/325000 (epoch: 235), loss: 0.154, time/batch: 0.102\n",
      "15340/325000 (epoch: 236), loss: 0.159, time/batch: 0.097\n",
      "15405/325000 (epoch: 237), loss: 0.171, time/batch: 0.100\n",
      "15470/325000 (epoch: 238), loss: 0.157, time/batch: 0.088\n",
      "15535/325000 (epoch: 239), loss: 0.152, time/batch: 0.093\n",
      "15600/325000 (epoch: 240), loss: 0.143, time/batch: 0.119\n",
      "15665/325000 (epoch: 241), loss: 0.147, time/batch: 0.088\n",
      "15730/325000 (epoch: 242), loss: 0.147, time/batch: 0.095\n",
      "15795/325000 (epoch: 243), loss: 0.154, time/batch: 0.085\n",
      "15860/325000 (epoch: 244), loss: 0.156, time/batch: 0.110\n",
      "15925/325000 (epoch: 245), loss: 0.160, time/batch: 0.090\n",
      "15990/325000 (epoch: 246), loss: 0.149, time/batch: 0.073\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "16055/325000 (epoch: 247), loss: 0.158, time/batch: 0.074\n",
      "16120/325000 (epoch: 248), loss: 0.153, time/batch: 0.085\n",
      "16185/325000 (epoch: 249), loss: 0.156, time/batch: 0.081\n",
      "16250/325000 (epoch: 250), loss: 0.147, time/batch: 0.080\n",
      "16315/325000 (epoch: 251), loss: 0.140, time/batch: 0.084\n",
      "16380/325000 (epoch: 252), loss: 0.122, time/batch: 0.105\n",
      "16445/325000 (epoch: 253), loss: 0.116, time/batch: 0.092\n",
      "16510/325000 (epoch: 254), loss: 0.121, time/batch: 0.095\n",
      "16575/325000 (epoch: 255), loss: 0.119, time/batch: 0.076\n",
      "16640/325000 (epoch: 256), loss: 0.110, time/batch: 0.084\n",
      "16705/325000 (epoch: 257), loss: 0.122, time/batch: 0.082\n",
      "16770/325000 (epoch: 258), loss: 0.121, time/batch: 0.076\n",
      "16835/325000 (epoch: 259), loss: 0.141, time/batch: 0.075\n",
      "16900/325000 (epoch: 260), loss: 0.129, time/batch: 0.073\n",
      "16965/325000 (epoch: 261), loss: 0.137, time/batch: 0.078\n",
      "17030/325000 (epoch: 262), loss: 0.138, time/batch: 0.072\n",
      "17095/325000 (epoch: 263), loss: 0.118, time/batch: 0.077\n",
      "17160/325000 (epoch: 264), loss: 0.109, time/batch: 0.077\n",
      "17225/325000 (epoch: 265), loss: 0.108, time/batch: 0.097\n",
      "17290/325000 (epoch: 266), loss: 0.115, time/batch: 0.109\n",
      "17355/325000 (epoch: 267), loss: 0.107, time/batch: 0.092\n",
      "17420/325000 (epoch: 268), loss: 0.118, time/batch: 0.076\n",
      "17485/325000 (epoch: 269), loss: 0.128, time/batch: 0.080\n",
      "17550/325000 (epoch: 270), loss: 0.141, time/batch: 0.078\n",
      "17615/325000 (epoch: 271), loss: 0.115, time/batch: 0.070\n",
      "17680/325000 (epoch: 272), loss: 0.106, time/batch: 0.080\n",
      "17745/325000 (epoch: 273), loss: 0.116, time/batch: 0.078\n",
      "17810/325000 (epoch: 274), loss: 0.115, time/batch: 0.129\n",
      "17875/325000 (epoch: 275), loss: 0.108, time/batch: 0.078\n",
      "17940/325000 (epoch: 276), loss: 0.124, time/batch: 0.080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to data/invisible_dragon/model.ckpt\n",
      "18005/325000 (epoch: 277), loss: 0.123, time/batch: 0.094\n",
      "18070/325000 (epoch: 278), loss: 0.104, time/batch: 0.092\n",
      "18135/325000 (epoch: 279), loss: 0.109, time/batch: 0.078\n",
      "18200/325000 (epoch: 280), loss: 0.093, time/batch: 0.105\n",
      "18265/325000 (epoch: 281), loss: 0.091, time/batch: 0.077\n",
      "18330/325000 (epoch: 282), loss: 0.098, time/batch: 0.071\n",
      "18395/325000 (epoch: 283), loss: 0.105, time/batch: 0.103\n",
      "18460/325000 (epoch: 284), loss: 0.092, time/batch: 0.080\n",
      "18525/325000 (epoch: 285), loss: 0.094, time/batch: 0.105\n",
      "18590/325000 (epoch: 286), loss: 0.095, time/batch: 0.078\n",
      "18655/325000 (epoch: 287), loss: 0.102, time/batch: 0.073\n",
      "18720/325000 (epoch: 288), loss: 0.105, time/batch: 0.078\n",
      "18785/325000 (epoch: 289), loss: 0.105, time/batch: 0.076\n",
      "18850/325000 (epoch: 290), loss: 0.089, time/batch: 0.072\n",
      "18915/325000 (epoch: 291), loss: 0.091, time/batch: 0.083\n",
      "18980/325000 (epoch: 292), loss: 0.091, time/batch: 0.075\n",
      "19045/325000 (epoch: 293), loss: 0.097, time/batch: 0.074\n",
      "19110/325000 (epoch: 294), loss: 0.087, time/batch: 0.079\n",
      "19175/325000 (epoch: 295), loss: 0.092, time/batch: 0.077\n",
      "19240/325000 (epoch: 296), loss: 0.090, time/batch: 0.104\n",
      "19305/325000 (epoch: 297), loss: 0.089, time/batch: 0.083\n",
      "19370/325000 (epoch: 298), loss: 0.093, time/batch: 0.073\n",
      "19435/325000 (epoch: 299), loss: 0.096, time/batch: 0.078\n",
      "19500/325000 (epoch: 300), loss: 0.092, time/batch: 0.073\n",
      "19565/325000 (epoch: 301), loss: 0.085, time/batch: 0.070\n",
      "19630/325000 (epoch: 302), loss: 0.082, time/batch: 0.079\n",
      "19695/325000 (epoch: 303), loss: 0.082, time/batch: 0.086\n",
      "19760/325000 (epoch: 304), loss: 0.091, time/batch: 0.079\n",
      "19825/325000 (epoch: 305), loss: 0.087, time/batch: 0.073\n",
      "19890/325000 (epoch: 306), loss: 0.079, time/batch: 0.073\n",
      "19955/325000 (epoch: 307), loss: 0.079, time/batch: 0.077\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "20020/325000 (epoch: 308), loss: 0.078, time/batch: 0.072\n",
      "20085/325000 (epoch: 309), loss: 0.088, time/batch: 0.074\n",
      "20150/325000 (epoch: 310), loss: 0.079, time/batch: 0.078\n",
      "20215/325000 (epoch: 311), loss: 0.081, time/batch: 0.070\n",
      "20280/325000 (epoch: 312), loss: 0.078, time/batch: 0.074\n",
      "20345/325000 (epoch: 313), loss: 0.073, time/batch: 0.077\n",
      "20410/325000 (epoch: 314), loss: 0.070, time/batch: 0.071\n",
      "20475/325000 (epoch: 315), loss: 0.070, time/batch: 0.076\n",
      "20540/325000 (epoch: 316), loss: 0.072, time/batch: 0.075\n",
      "20605/325000 (epoch: 317), loss: 0.068, time/batch: 0.072\n",
      "20670/325000 (epoch: 318), loss: 0.079, time/batch: 0.082\n",
      "20735/325000 (epoch: 319), loss: 0.073, time/batch: 0.081\n",
      "20800/325000 (epoch: 320), loss: 0.075, time/batch: 0.077\n",
      "20865/325000 (epoch: 321), loss: 0.076, time/batch: 0.076\n",
      "20930/325000 (epoch: 322), loss: 0.068, time/batch: 0.076\n",
      "20995/325000 (epoch: 323), loss: 0.069, time/batch: 0.100\n",
      "21060/325000 (epoch: 324), loss: 0.064, time/batch: 0.085\n",
      "21125/325000 (epoch: 325), loss: 0.064, time/batch: 0.076\n",
      "21190/325000 (epoch: 326), loss: 0.074, time/batch: 0.079\n",
      "21255/325000 (epoch: 327), loss: 0.072, time/batch: 0.078\n",
      "21320/325000 (epoch: 328), loss: 0.069, time/batch: 0.089\n",
      "21385/325000 (epoch: 329), loss: 0.069, time/batch: 0.091\n",
      "21450/325000 (epoch: 330), loss: 0.074, time/batch: 0.082\n",
      "21515/325000 (epoch: 331), loss: 0.073, time/batch: 0.073\n",
      "21580/325000 (epoch: 332), loss: 0.070, time/batch: 0.113\n",
      "21645/325000 (epoch: 333), loss: 0.072, time/batch: 0.079\n",
      "21710/325000 (epoch: 334), loss: 0.067, time/batch: 0.099\n",
      "21775/325000 (epoch: 335), loss: 0.067, time/batch: 0.094\n",
      "21840/325000 (epoch: 336), loss: 0.065, time/batch: 0.071\n",
      "21905/325000 (epoch: 337), loss: 0.063, time/batch: 0.075\n",
      "21970/325000 (epoch: 338), loss: 0.060, time/batch: 0.078\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "22035/325000 (epoch: 339), loss: 0.065, time/batch: 0.077\n",
      "22100/325000 (epoch: 340), loss: 0.065, time/batch: 0.074\n",
      "22165/325000 (epoch: 341), loss: 0.068, time/batch: 0.078\n",
      "22230/325000 (epoch: 342), loss: 0.060, time/batch: 0.082\n",
      "22295/325000 (epoch: 343), loss: 0.057, time/batch: 0.100\n",
      "22360/325000 (epoch: 344), loss: 0.054, time/batch: 0.078\n",
      "22425/325000 (epoch: 345), loss: 0.058, time/batch: 0.075\n",
      "22490/325000 (epoch: 346), loss: 0.062, time/batch: 0.078\n",
      "22555/325000 (epoch: 347), loss: 0.057, time/batch: 0.076\n",
      "22620/325000 (epoch: 348), loss: 0.051, time/batch: 0.079\n",
      "22685/325000 (epoch: 349), loss: 0.046, time/batch: 0.077\n",
      "22750/325000 (epoch: 350), loss: 0.047, time/batch: 0.071\n",
      "22815/325000 (epoch: 351), loss: 0.051, time/batch: 0.110\n",
      "22880/325000 (epoch: 352), loss: 0.049, time/batch: 0.090\n",
      "22945/325000 (epoch: 353), loss: 0.054, time/batch: 0.072\n",
      "23010/325000 (epoch: 354), loss: 0.045, time/batch: 0.077\n",
      "23075/325000 (epoch: 355), loss: 0.048, time/batch: 0.073\n",
      "23140/325000 (epoch: 356), loss: 0.041, time/batch: 0.074\n",
      "23205/325000 (epoch: 357), loss: 0.043, time/batch: 0.082\n",
      "23270/325000 (epoch: 358), loss: 0.044, time/batch: 0.085\n",
      "23335/325000 (epoch: 359), loss: 0.040, time/batch: 0.083\n",
      "23400/325000 (epoch: 360), loss: 0.042, time/batch: 0.075\n",
      "23465/325000 (epoch: 361), loss: 0.043, time/batch: 0.073\n",
      "23530/325000 (epoch: 362), loss: 0.043, time/batch: 0.078\n",
      "23595/325000 (epoch: 363), loss: 0.038, time/batch: 0.072\n",
      "23660/325000 (epoch: 364), loss: 0.042, time/batch: 0.074\n",
      "23725/325000 (epoch: 365), loss: 0.041, time/batch: 0.077\n",
      "23790/325000 (epoch: 366), loss: 0.039, time/batch: 0.076\n",
      "23855/325000 (epoch: 367), loss: 0.043, time/batch: 0.084\n",
      "23920/325000 (epoch: 368), loss: 0.036, time/batch: 0.083\n",
      "23985/325000 (epoch: 369), loss: 0.038, time/batch: 0.079\n",
      "model saved to data/invisible_dragon/model.ckpt\n",
      "24050/325000 (epoch: 370), loss: 0.042, time/batch: 0.076\n",
      "24115/325000 (epoch: 371), loss: 0.046, time/batch: 0.078\n",
      "24180/325000 (epoch: 372), loss: 0.040, time/batch: 0.077\n",
      "24245/325000 (epoch: 373), loss: 0.040, time/batch: 0.109\n",
      "24310/325000 (epoch: 374), loss: 0.038, time/batch: 0.085\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-36cc09519909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# PRINT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yj/.virtualenvs/lecture/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yj/.virtualenvs/lecture/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yj/.virtualenvs/lecture/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/yj/.virtualenvs/lecture/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yj/.virtualenvs/lecture/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "summary_writer = tf.summary.FileWriter(save_dir\n",
    "                    , graph=sess.graph)\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "for e in range(num_epochs): # for all epochs\n",
    "    # LEARNING RATE SCHEDULING \n",
    "    sess.run(tf.assign(lr, learning_rate * (decay_rate ** e)))\n",
    "\n",
    "    data_loader.reset_batch_pointer()\n",
    "    state = sess.run(initial_state)\n",
    "    for b in range(data_loader.num_batches):\n",
    "        start = time.time()\n",
    "        x, y = data_loader.next_batch()\n",
    "        feed = {input_data: x, targets: y, initial_state: state}\n",
    "        # Train!\n",
    "        train_loss, state, _ = sess.run([cost, final_state, optm], feed)\n",
    "        end = time.time()\n",
    "        # PRINT \n",
    "        if b % 100 == 0:\n",
    "            print (\"%d/%d (epoch: %d), loss: %.3f, time/batch: %.3f\"  \n",
    "                   % (e * data_loader.num_batches + b\n",
    "                    , num_epochs * data_loader.num_batches\n",
    "                    , e, train_loss, end - start))\n",
    "        # SAVE MODEL\n",
    "        if (e * data_loader.num_batches + b) % save_every == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path\n",
    "                       , global_step = e * data_loader.num_batches + b)\n",
    "            print(\"model saved to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
